{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Having a Prior look over existing trained dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buyer_intent       134\n",
       "buyer_verbs         76\n",
       "products           653\n",
       "seller_intent      511\n",
       "seller_verbs       200\n",
       "ad_campaigns        42\n",
       "negative_verbs      79\n",
       "intent_suffixes     26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to know what is the status of existing dataset.\n",
    "df=pd.read_excel(\"training_intent_samples.xlsx\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training Starts here: txt to tabular data** ---------------- >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Phase 1 of 2* of Training Data: \n",
    "Creating patterns from WORD BANK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Description:\n",
    "* Features 8\n",
    "* Label: 9th column\n",
    "* Dataset name:  dataset_trained.xlsx.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expression for pattern detection\n",
    "import openpyxl as op # For using tabular data.\n",
    "from tqdm import tqdm # Special Library for showing Progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Buyers and Sellers data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Buyers and Sellers data for training\n",
    "wb=op.load_workbook(r\"training_intent_samples.xlsx\")\n",
    "ws=wb.active\n",
    "\n",
    "buyers= []\n",
    "buyer_verbs=[]\n",
    "products=[]\n",
    "sellers = []\n",
    "seller_verbs=[]\n",
    "ad_camp=[]\n",
    "negations=[]\n",
    "intent_suffixes=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting buyers and seller lists of WORD BANK to predict the intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting buyers and seller lists of WORD BANK to predict the intents\n",
    "for row in ws.iter_rows():\n",
    "    if row[0].value is not None:\n",
    "        buyers.append(str(row[0].value))\n",
    "        \n",
    "    if row[1].value is not None:\n",
    "        buyer_verbs.append(str(row[1].value))\n",
    "\n",
    "    if row[2].value is not None:\n",
    "        products.append(str(row[2].value))\n",
    "        \n",
    "    if row[3].value is not None:\n",
    "        sellers.append(str(row[3].value))\n",
    "        \n",
    "    if row[4].value is not None:\n",
    "        seller_verbs.append(str(row[4].value))\n",
    "        \n",
    "    if row[5].value is not None:\n",
    "        ad_camp.append(str(row[5].value))\n",
    "        \n",
    "    if row[6].value is not None:\n",
    "        negations.append(str(row[6].value))\n",
    "        \n",
    "    if row[7].value is not None:\n",
    "        intent_suffixes.append(str(row[7].value))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Regex patterns to match later for 4 Classes: \n",
    "Buyer, Seller, Buyer Seller and Nothing Intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating patterns for regular expression for buyer and seller intents\n",
    "buyer_pattern = r'\\b(?:' + '|'.join(map(re.escape, buyers)) + r')\\b'\n",
    "buyer_verbs_pattern = r'\\b(?:' + '|'.join(map(re.escape, buyer_verbs)) + r')\\b'\n",
    "products_pattern = r'\\b(?:' + '|'.join(map(re.escape, products)) + r')'\n",
    "sellers_pattern = r'\\b(?:' + '|'.join(map(re.escape, sellers)) + r')\\b'\n",
    "seller_verbs_pattern = r'\\b(?:' + '|'.join(map(re.escape, seller_verbs)) + r')\\b'\n",
    "ad_camp_pattern = r'\\b(?:' + '|'.join(map(re.escape, ad_camp)) + r')\\b'\n",
    "negations_pattern = r'\\b(?:' + '|'.join(map(re.escape, negations)) + r')\\b' \n",
    "intent_suffixes_pattern = r'\\b(?:' + '|'.join(map(re.escape, intent_suffixes)) + r')' \n",
    "\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Training Data : Raw Sentences\n",
    "There are options to update data set:\n",
    "1. Using individual sentences, and\n",
    "2. Using one or more .txt files having many sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading training Data\n",
    "# Choice \n",
    "choice=int(input('''For sentence input select \"1\" \n",
    "For txt(s) files select \"2\" \n",
    ">>> ''').strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Phase 2 of 2* of Training Data:\n",
    "Creating a function evaluate( line, label ): \n",
    "For processing each sentences one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(line,label):\n",
    "    text=line\n",
    "    \n",
    "    # setting row back to 0\n",
    "    row=0 \n",
    "    wb2=op.load_workbook(r\"G:\\GitClone\\NLP\\NLP\\dataset_trained.xlsx.xlsx\")\n",
    "    ws2 = wb2.active\n",
    "    # Adding Header to all column in trained dataset\n",
    "    if ws2.cell(row+1,1).value!=\"buyer\":\n",
    "        ws2.cell(row+1,1).value=\"buyer\"\n",
    "        ws2.cell(row+1,2).value=\"seller\"\n",
    "        ws2.cell(row+1,3).value=\"seller_verb\"\n",
    "        ws2.cell(row+1,4).value=\"buyer_verb\"\n",
    "        ws2.cell(row+1,5).value=\"products\"\n",
    "        ws2.cell(row+1,6).value=\"ad_campaign\"\n",
    "        ws2.cell(row+1,7).value=\"intent_sufixes\"\n",
    "        ws2.cell(row+1,8).value=\"negations\"\n",
    "        ws2.cell(row+1,9).value=\"label\"\n",
    "              \n",
    "    row= ws2.max_row # setting row to next empty cell\n",
    "    \n",
    "    # Finding all occurrences using the re module for patterns\n",
    "    buyer_match = re.findall(buyer_pattern, text, flags=re.IGNORECASE)\n",
    "    buyer_verbs_match = re.findall(buyer_verbs_pattern, text, flags=re.IGNORECASE)\n",
    "    products_match = re.findall(products_pattern, text, flags=re.IGNORECASE)\n",
    "    seller_match = re.findall(sellers_pattern, text, flags=re.IGNORECASE)\n",
    "    seller_verbs_match = re.findall(seller_verbs_pattern, text, flags=re.IGNORECASE)\n",
    "    ad_camp_match = re.findall(ad_camp_pattern, text, flags=re.IGNORECASE)\n",
    "    negations_match = re.findall(negations_pattern, text, flags=re.IGNORECASE)\n",
    "    intent_suffixes_match= re.findall(intent_suffixes_pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "    B=len(buyer_match)\n",
    "    S=len(seller_match)\n",
    "    BV=len(buyer_verbs_match)\n",
    "    SV=len(seller_verbs_match)\n",
    "    P=len(products_match)\n",
    "    Ad=len(ad_camp_match)\n",
    "    IS=len(intent_suffixes_match)\n",
    "    PN=len(negations_match)\n",
    "    \n",
    "    ws2.cell(row+1,1).value=B\n",
    "    ws2.cell(row+1,2).value=S\n",
    "    ws2.cell(row+1,3).value=SV\n",
    "    ws2.cell(row+1,4).value=BV\n",
    "    ws2.cell(row+1,5).value=P\n",
    "    ws2.cell(row+1,6).value=Ad \n",
    "    ws2.cell(row+1,7).value=IS\n",
    "    ws2.cell(row+1,8).value=PN\n",
    "\n",
    "\n",
    "    # adding label to the training data\n",
    "    ws2.cell(row+1,9).value=label.upper()\n",
    "\n",
    "    row+=1\n",
    "    wb2.save(r\"G:\\GitClone\\NLP\\NLP\\dataset_trained.xlsx.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interacting with User for raw data feed: \n",
    "User has to provide either _INDIVIDUAL SENTENCE_ or _.txt FILE(s)_ with their repective Labels: B, S, BS or N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [00:39<00:00, 12.57iteration/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:28<00:00,  5.68iteration/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [02:26<00:00,  3.41iteration/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [03:15<00:00,  2.55iteration/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed ðŸŸ¢\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_files_n_labels=[] # for taking number of file's paths and their respective labels\n",
    "\n",
    "if choice==1:\n",
    "    training_file_txt=input(\"Waiting for your sentence>>>\")\n",
    "    label=input(\"Select Labels: B, S, BS or N >>>\"  ).strip()\n",
    "    evaluate(training_file_txt,label)\n",
    "    print(\"Completed ðŸŸ¢\")\n",
    "    \n",
    "    \n",
    "elif choice==2:\n",
    "    num=int(input(\"Number of files >>>  \").strip())\n",
    "    \n",
    "    for c in range(num):\n",
    "        path=input(f\"File path #{c+1} >>>\").strip()\n",
    "        label=input(\"Select Labels:  B, S, BS or N >>>\").strip().upper()\n",
    "        training_files_n_labels.append([path,label])\n",
    "        \n",
    "    for f in range(num): # num is number of files that user feeded for training\n",
    "        training_file_txt=training_files_n_labels[f][0]\n",
    "        label=training_files_n_labels[f][1]\n",
    "        \n",
    "        with open(training_file_txt, 'r') as file: # just to find total length for progress bar\n",
    "            length=len([line for line in file])\n",
    "                \n",
    "                        \n",
    "        with open(training_file_txt, 'r') as file:\n",
    "            progress_bar = tqdm(total=length, unit=\"iteration\") # progress Bar\n",
    "            for line in file:\n",
    "                progress_bar.update(1)\n",
    "                evaluate(line.strip(),label)\n",
    "                \n",
    "            progress_bar.close()\n",
    "                \n",
    "    print(\"Completed ðŸŸ¢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Part Ends Here. Now Trained Dataset is ready to be used. I has encoded features for sentence's description. --------<<>>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
